{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64c0a35-599e-4698-b635-2a4e6955be0c",
   "metadata": {},
   "source": [
    "# üß™ Multinomial Toxicity Classification (Levels 1‚Äì5)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a **5-class toxicity classifier** using semantic embeddings from a toxicity-tuned DeBERTa-v3 model. The pipeline consists of:\n",
    "\n",
    "| Step | Task | Description |\n",
    "|------|------|-------------|\n",
    "| 0 | Setup | Environment configuration and reproducibility |\n",
    "| 1 | Data Ingestion | Load ToxiGen dataset and format labels |\n",
    "| 2 | Text Normalization | Lightweight preprocessing preserving emotional cues |\n",
    "| 3 | Semantic Embeddings | Extract 768-dim vectors via DeBERTa-v3-toxicity |\n",
    "| 4 | Feature Engineering | Standardize embeddings for classifier input |\n",
    "| 5 | Model Training | Multinomial logistic regression with evaluation |\n",
    "| 6 | Export Artifacts | Save model, scaler, and tokenizer for inference |\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Approach?\n",
    "\n",
    "Toxic language is nuanced‚Äîit spans explicit insults, subtle sarcasm, passive aggression, and coded hate. A single binary classifier often fails to capture this spectrum. By framing toxicity as a **5-level ordinal problem**, we preserve granularity while enabling more interpretable predictions.\n",
    "\n",
    "We use **DeBERTa-v3** fine-tuned on toxicity data because:\n",
    "- Its disentangled attention captures nuanced context better than BERT\n",
    "- Pre-training on toxicity corpora encodes harmful language patterns directly\n",
    "- The 768-dimensional embeddings are rich enough for downstream classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf72009-c423-49eb-834d-5643e6c18063",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Task 00 ‚Äî Environment Setup & Reproducibility\n",
    "\n",
    "We establish a reproducible environment by:\n",
    "1. **Fixing random seeds** across Python, NumPy, and PyTorch\n",
    "2. **Importing all dependencies** upfront for clarity\n",
    "3. **Defining utility functions** (logging, device selection)\n",
    "\n",
    "### Dependencies\n",
    "```\n",
    "torch              # Neural network backend\n",
    "transformers       # HuggingFace models (DeBERTa)\n",
    "datasets           # HuggingFace data loading (ToxiGen)\n",
    "scikit-learn       # Logistic regression, metrics, scaling\n",
    "pandas, numpy      # Data manipulation\n",
    "joblib             # Model serialization\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f9300d-ded0-48fb-8022-8c1d67dd5476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Environment ready.\n",
      "     Device: cuda\n",
      "     Random seed: 42\n",
      "     Output directory: ./toxicity_model_artifacts\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 0 ‚Äî ENVIRONMENT SETUP & REPRODUCIBILITY\n",
    "# ============================================================================\n",
    "\n",
    "# --- Standard Library ---\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Deep Learning ---\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# --- Machine Learning ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# --- Data Loading ---\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Model Persistence ---\n",
    "import joblib\n",
    "\n",
    "# Suppress non-critical warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility Settings\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Device Configuration\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------\n",
    "# Logging Utility\n",
    "# -------------------------\n",
    "def log(msg: str):\n",
    "    \"\"\"Simple timestamped logging.\"\"\"\n",
    "    print(msg, flush=True)\n",
    "\n",
    "# -------------------------\n",
    "# Configuration Constants\n",
    "# -------------------------\n",
    "MODEL_NAME = \"sileod/deberta-v3-base-tasksource-toxicity\"\n",
    "EMBEDDING_DIM = 768\n",
    "MAX_SEQ_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Output paths for saved artifacts\n",
    "OUTPUT_DIR = \"./toxicity_model_artifacts\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "log(f\"[OK] Environment ready.\")\n",
    "log(f\"     Device: {DEVICE}\")\n",
    "log(f\"     Random seed: {SEED}\")\n",
    "log(f\"     Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09397382-a92c-45dd-b8ce-856b6820431b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Task 01 ‚Äî Data Ingestion & Label Formatting\n",
    "\n",
    "### Dataset: ToxiGen\n",
    "ToxiGen is a large-scale dataset of machine-generated toxic and benign statements, with human annotations for toxicity severity.\n",
    "\n",
    "### Label Transformation\n",
    "Raw human toxicity scores are continuous (e.g., 3.7, 4.2). We convert them to discrete classes:\n",
    "\n",
    "$$\n",
    "y = \\text{clip}\\left( \\text{round}(t), 1, 5 \\right)\n",
    "$$\n",
    "\n",
    "This produces **5 ordinal classes**:\n",
    "\n",
    "| Class | Interpretation |\n",
    "|-------|----------------|\n",
    "| 1 | Non-toxic / Neutral |\n",
    "| 2 | Mildly problematic |\n",
    "| 3 | Moderately toxic |\n",
    "| 4 | Clearly toxic |\n",
    "| 5 | Severely toxic / Hate speech |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdd1b4f-3c9a-4669-a67a-a106221d2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1 ‚Äî DATA INGESTION & LABEL FORMATTING\n",
      "======================================================================\n",
      "\n",
      "[1.1] Loading ToxiGen dataset from HuggingFace...\n",
      "     Training samples: 8,960\n",
      "     Test samples:     940\n",
      "\n",
      "[1.2] Retained columns: ['text', 'toxicity_human']\n",
      "\n",
      "[1.3] Label distribution (TRAIN):\n",
      "     Class 1: 3,230 samples (36.0%)\n",
      "     Class 2: 1,965 samples (21.9%)\n",
      "     Class 3: 1,093 samples (12.2%)\n",
      "     Class 4: 1,145 samples (12.8%)\n",
      "     Class 5: 1,527 samples (17.0%)\n",
      "\n",
      "[OK] Data ingestion complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1 ‚Äî DATA INGESTION & LABEL FORMATTING\n",
    "# ============================================================================\n",
    "log(\"\\n\" + \"=\"*70)\n",
    "log(\"STEP 1 ‚Äî DATA INGESTION & LABEL FORMATTING\")\n",
    "log(\"=\"*70)\n",
    "\n",
    "# ---------------------------------\n",
    "# 1.1 Load ToxiGen from HuggingFace\n",
    "# ---------------------------------\n",
    "log(\"\\n[1.1] Loading ToxiGen dataset from HuggingFace...\")\n",
    "\n",
    "ds_train = load_dataset(\"toxigen/toxigen-data\", split=\"train\")\n",
    "ds_test = load_dataset(\"toxigen/toxigen-data\", split=\"test\")\n",
    "\n",
    "# Convert to pandas DataFrames for easier manipulation\n",
    "df_train = ds_train.to_pandas()\n",
    "df_test = ds_test.to_pandas()\n",
    "\n",
    "log(f\"     Training samples: {len(df_train):,}\")\n",
    "log(f\"     Test samples:     {len(df_test):,}\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 1.2 Extract Required Columns\n",
    "# ---------------------------------\n",
    "# We only need the text and human toxicity score\n",
    "REQUIRED_COLS = [\"text\", \"toxicity_human\"]\n",
    "df_train = df_train[REQUIRED_COLS].copy()\n",
    "df_test = df_test[REQUIRED_COLS].copy()\n",
    "\n",
    "log(f\"\\n[1.2] Retained columns: {REQUIRED_COLS}\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 1.3 Convert Continuous ‚Üí Discrete Labels\n",
    "# ---------------------------------\n",
    "def continuous_to_ordinal(score: float) -> int:\n",
    "    \"\"\"\n",
    "    Convert continuous toxicity score to ordinal class {1, 2, 3, 4, 5}.\n",
    "    \n",
    "    Formula: y = clip(round(score), 1, 5)\n",
    "    \n",
    "    Args:\n",
    "        score: Raw toxicity score (typically 1.0 to 5.0)\n",
    "    \n",
    "    Returns:\n",
    "        Integer class label in {1, 2, 3, 4, 5}\n",
    "    \"\"\"\n",
    "    y = round(float(score))     # Round to nearest integer\n",
    "    y = max(1, min(5, y))       # Clip to valid range [1, 5]\n",
    "    return int(y)\n",
    "\n",
    "df_train[\"toxicity_label\"] = df_train[\"toxicity_human\"].apply(continuous_to_ordinal)\n",
    "df_test[\"toxicity_label\"] = df_test[\"toxicity_human\"].apply(continuous_to_ordinal)\n",
    "\n",
    "log(\"\\n[1.3] Label distribution (TRAIN):\")\n",
    "label_counts = df_train[\"toxicity_label\"].value_counts().sort_index()\n",
    "for label, count in label_counts.items():\n",
    "    pct = 100 * count / len(df_train)\n",
    "    log(f\"     Class {label}: {count:,} samples ({pct:.1f}%)\")\n",
    "\n",
    "log(\"\\n[OK] Data ingestion complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5da837-42fa-458e-a086-a3f602896f44",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Task 02 ‚Äî Text Normalization\n",
    "\n",
    "### Philosophy: Preserve Emotional Signals\n",
    "\n",
    "Unlike standard NLP preprocessing, **toxicity detection requires preserving expressive features**:\n",
    "\n",
    "| Feature | Example | Signal |\n",
    "|---------|---------|--------|\n",
    "| Repeated punctuation | `\"IDIOT!!!\"` | Emotional intensity |\n",
    "| Ellipsis | `\"wow... nice job\"` | Sarcasm / passive aggression |\n",
    "| ALL CAPS | `\"YOU ARE TRASH\"` | Shouting / aggression |\n",
    "| Emojis | `\"üòí\"` | Tone modifiers |\n",
    "\n",
    "### Our Normalization Strategy\n",
    "\n",
    "| Operation | Applied? | Reason |\n",
    "|-----------|----------|--------|\n",
    "| Lowercase | ‚úÖ Yes | Reduces vocabulary sparsity |\n",
    "| Whitespace normalization | ‚úÖ Yes | Removes noise |\n",
    "| Remove punctuation | ‚ùå No | Preserves emotional cues |\n",
    "| Remove emojis | ‚ùå No | Preserves tone indicators |\n",
    "| Remove stopwords | ‚ùå No | Context-dependent meaning |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65936531-1ab7-461a-b0dd-fb09083fed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2 ‚Äî TEXT NORMALIZATION\n",
      "======================================================================\n",
      "\n",
      "[2.1] Applying normalization to TRAIN set...\n",
      "[2.2] Applying normalization to TEST set...\n",
      "\n",
      "[2.3] Sample normalized entries:\n",
      "--------------------------------------------------\n",
      "Original:   asians are trying to take over the world with their karate skills...\n",
      "Normalized: asians are trying to take over the world with their karate skills...\n",
      "\n",
      "Original:   i have never seen a hotter lineup in my life than in mexico city on the day of i...\n",
      "Normalized: i have never seen a hotter lineup in my life than in mexico city on the day of i...\n",
      "\n",
      "Original:   muslims deseve to die...\n",
      "Normalized: muslims deseve to die...\n",
      "\n",
      "[OK] Text normalization complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2 ‚Äî TEXT NORMALIZATION\n",
    "# ============================================================================\n",
    "log(\"\\n\" + \"=\"*70)\n",
    "log(\"STEP 2 ‚Äî TEXT NORMALIZATION\")\n",
    "log(\"=\"*70)\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply minimal text normalization that preserves emotional signals.\n",
    "    \n",
    "    Operations:\n",
    "        1. Convert to lowercase (reduces sparsity)\n",
    "        2. Collapse multiple whitespace to single space\n",
    "        3. Strip leading/trailing whitespace\n",
    "    \n",
    "    Preserved:\n",
    "        - Punctuation (!!!, ..., etc.)\n",
    "        - Emojis and special characters\n",
    "        - Numbers\n",
    "    \n",
    "    Args:\n",
    "        text: Raw input string\n",
    "    \n",
    "    Returns:\n",
    "        Normalized string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()                        # Lowercase\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()   # Normalize whitespace\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply normalization to both datasets\n",
    "log(\"\\n[2.1] Applying normalization to TRAIN set...\")\n",
    "df_train[\"text_norm\"] = df_train[\"text\"].apply(normalize_text)\n",
    "\n",
    "log(\"[2.2] Applying normalization to TEST set...\")\n",
    "df_test[\"text_norm\"] = df_test[\"text\"].apply(normalize_text)\n",
    "\n",
    "# Display samples\n",
    "log(\"\\n[2.3] Sample normalized entries:\")\n",
    "log(\"-\" * 50)\n",
    "for idx in range(3):\n",
    "    orig = df_train.iloc[idx][\"text\"][:80]\n",
    "    norm = df_train.iloc[idx][\"text_norm\"][:80]\n",
    "    log(f\"Original:   {orig}...\")\n",
    "    log(f\"Normalized: {norm}...\")\n",
    "    log(\"\")\n",
    "\n",
    "log(\"[OK] Text normalization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1817b7a-57a9-495f-a4fe-4839d90661aa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Task 03 ‚Äî Toxicity-Aware Semantic Embeddings\n",
    "\n",
    "### Model: DeBERTa-v3-base (Toxicity Fine-tuned)\n",
    "\n",
    "We use `sileod/deberta-v3-base-tasksource-toxicity`, a DeBERTa model fine-tuned on toxicity classification tasks. This provides:\n",
    "\n",
    "- **Disentangled attention**: Separates content and position encodings for richer context\n",
    "- **Toxicity-specific representations**: Hidden states shaped by harmful language patterns\n",
    "- **768-dimensional embeddings**: Rich feature space for downstream classification\n",
    "\n",
    "### Embedding Extraction\n",
    "\n",
    "Given input text $x$, the model produces contextualized token representations:\n",
    "\n",
    "$$\n",
    "H = \\text{DeBERTa}(x) \\in \\mathbb{R}^{T \\times 768}\n",
    "$$\n",
    "\n",
    "where $T$ is the sequence length. We apply **mean pooling** to obtain a fixed-size sentence embedding:\n",
    "\n",
    "$$\n",
    "e(x) = \\frac{1}{T} \\sum_{t=1}^{T} H_t \\in \\mathbb{R}^{768}\n",
    "$$\n",
    "\n",
    "### Why Mean Pooling?\n",
    "\n",
    "| Strategy | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| [CLS] token | Fast, single vector | May miss distributed toxicity signals |\n",
    "| Max pooling | Captures strongest signals | Sensitive to outliers |\n",
    "| **Mean pooling** | Balanced representation | Slightly diluted by padding |\n",
    "\n",
    "Mean pooling provides a stable, holistic representation of the entire input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928a357c-3f85-4f21-84ea-1e2368345c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3 ‚Äî SEMANTIC EMBEDDINGS (DeBERTa-v3 Toxicity)\n",
      "======================================================================\n",
      "\n",
      "[3.1] Loading model: sileod/deberta-v3-base-tasksource-toxicity\n",
      "      Device: cuda\n",
      "      Model loaded successfully.\n",
      "      Hidden size: 768\n",
      "\n",
      "[3.2] Generating embeddings for TRAIN set...\n",
      "      Processing batch 50/280...\n",
      "      Processing batch 100/280...\n",
      "      Processing batch 150/280...\n",
      "      Processing batch 200/280...\n",
      "      Processing batch 250/280...\n",
      "      Shape: (8960, 768)\n",
      "\n",
      "[3.3] Generating embeddings for TEST set...\n",
      "      Shape: (940, 768)\n",
      "\n",
      "[OK] Embedding extraction complete.\n",
      "     Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3 ‚Äî SEMANTIC EMBEDDINGS (DeBERTa-v3 Toxicity)\n",
    "# ============================================================================\n",
    "log(\"\\n\" + \"=\"*70)\n",
    "log(\"STEP 3 ‚Äî SEMANTIC EMBEDDINGS (DeBERTa-v3 Toxicity)\")\n",
    "log(\"=\"*70)\n",
    "\n",
    "# ---------------------------------\n",
    "# 3.1 Load Pre-trained Model\n",
    "# ---------------------------------\n",
    "log(f\"\\n[3.1] Loading model: {MODEL_NAME}\")\n",
    "log(f\"      Device: {DEVICE}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()  # Set to evaluation mode (disables dropout)\n",
    "\n",
    "log(f\"      Model loaded successfully.\")\n",
    "log(f\"      Hidden size: {model.config.hidden_size}\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 3.2 Define Embedding Function\n",
    "# ---------------------------------\n",
    "def extract_embeddings(\n",
    "    texts: List[str],\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    max_length: int = MAX_SEQ_LENGTH,\n",
    "    show_progress: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract mean-pooled DeBERTa embeddings for a list of texts.\n",
    "    \n",
    "    Process:\n",
    "        1. Tokenize text with padding and truncation\n",
    "        2. Forward pass through DeBERTa (no gradients)\n",
    "        3. Mean-pool over token dimension\n",
    "    \n",
    "    Args:\n",
    "        texts: List of input strings\n",
    "        batch_size: Number of samples per batch\n",
    "        max_length: Maximum token sequence length\n",
    "        show_progress: Whether to log progress\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array of shape (N, 768)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    n_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        if show_progress and batch_num % 50 == 0:\n",
    "            log(f\"      Processing batch {batch_num}/{n_batches}...\")\n",
    "        \n",
    "        # Tokenize batch\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Forward pass (no gradient computation)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            hidden_states = outputs.last_hidden_state  # (B, T, 768)\n",
    "            \n",
    "            # Mean pooling over sequence dimension\n",
    "            embeddings = hidden_states.mean(dim=1)     # (B, 768)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# ---------------------------------\n",
    "# 3.3 Generate Embeddings\n",
    "# ---------------------------------\n",
    "log(\"\\n[3.2] Generating embeddings for TRAIN set...\")\n",
    "train_embeddings = extract_embeddings(df_train[\"text_norm\"].tolist())\n",
    "log(f\"      Shape: {train_embeddings.shape}\")\n",
    "\n",
    "log(\"\\n[3.3] Generating embeddings for TEST set...\")\n",
    "test_embeddings = extract_embeddings(df_test[\"text_norm\"].tolist())\n",
    "log(f\"      Shape: {test_embeddings.shape}\")\n",
    "\n",
    "log(f\"\\n[OK] Embedding extraction complete.\")\n",
    "log(f\"     Embedding dimension: {train_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f7609-8b23-474a-9e03-178964a9093e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Task 04 ‚Äî Feature Standardization\n",
    "\n",
    "### Why Standardize?\n",
    "\n",
    "Many classifiers (logistic regression, SVM, neural nets) perform better when features are:\n",
    "- **Zero-centered**: Mean = 0\n",
    "- **Unit variance**: Std = 1\n",
    "\n",
    "This prevents features with larger magnitudes from dominating the optimization.\n",
    "\n",
    "### StandardScaler Transformation\n",
    "\n",
    "For each feature dimension $j$:\n",
    "\n",
    "$$\n",
    "z'_j = \\frac{z_j - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "\n",
    "where $\\mu_j$ and $\\sigma_j$ are computed **only on the training set**.\n",
    "\n",
    "### ‚ö†Ô∏è Critical: Avoiding Data Leakage\n",
    "\n",
    "```python\n",
    "# CORRECT: Fit on train, transform both\n",
    "scaler.fit(X_train)           # Learn Œº, œÉ from training data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # Apply SAME Œº, œÉ\n",
    "\n",
    "# WRONG: Fitting on test data leaks information!\n",
    "# scaler.fit(X_test)  # ‚ùå NEVER DO THIS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193f3928-fe93-412c-a8ea-0c6d24cd15fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4 ‚Äî FEATURE STANDARDIZATION\n",
      "======================================================================\n",
      "\n",
      "[4.1] Assembling feature matrices...\n",
      "      X_train shape: (8960, 768)\n",
      "      X_test shape:  (940, 768)\n",
      "      y_train shape: (8960,)\n",
      "      y_test shape:  (940,)\n",
      "\n",
      "[4.2] Fitting StandardScaler on training data...\n",
      "      Scaler fitted.\n",
      "      Mean range: [-4.0460, 4.1614]\n",
      "      Std range:  [0.1571, 4.2725]\n",
      "\n",
      "[4.3] Verifying standardization (TRAIN):\n",
      "      Mean of features: -0.000000 (should be ~0)\n",
      "      Std of features:  1.000000 (should be ~1)\n",
      "\n",
      "[OK] Feature standardization complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4 ‚Äî FEATURE STANDARDIZATION\n",
    "# ============================================================================\n",
    "log(\"\\n\" + \"=\"*70)\n",
    "log(\"STEP 4 ‚Äî FEATURE STANDARDIZATION\")\n",
    "log(\"=\"*70)\n",
    "\n",
    "# ---------------------------------\n",
    "# 4.1 Prepare Feature Matrices\n",
    "# ---------------------------------\n",
    "log(\"\\n[4.1] Assembling feature matrices...\")\n",
    "\n",
    "X_train_raw = train_embeddings  # (N_train, 768)\n",
    "X_test_raw = test_embeddings    # (N_test, 768)\n",
    "\n",
    "y_train = df_train[\"toxicity_label\"].values\n",
    "y_test = df_test[\"toxicity_label\"].values\n",
    "\n",
    "log(f\"      X_train shape: {X_train_raw.shape}\")\n",
    "log(f\"      X_test shape:  {X_test_raw.shape}\")\n",
    "log(f\"      y_train shape: {y_train.shape}\")\n",
    "log(f\"      y_test shape:  {y_test.shape}\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 4.2 Fit Scaler on Training Data\n",
    "# ---------------------------------\n",
    "log(\"\\n[4.2] Fitting StandardScaler on training data...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)  # Fit and transform\n",
    "X_test = scaler.transform(X_test_raw)        # Transform only (no fit!)\n",
    "\n",
    "log(f\"      Scaler fitted.\")\n",
    "log(f\"      Mean range: [{scaler.mean_.min():.4f}, {scaler.mean_.max():.4f}]\")\n",
    "log(f\"      Std range:  [{scaler.scale_.min():.4f}, {scaler.scale_.max():.4f}]\")\n",
    "\n",
    "# Verify standardization\n",
    "log(\"\\n[4.3] Verifying standardization (TRAIN):\")\n",
    "log(f\"      Mean of features: {X_train.mean():.6f} (should be ~0)\")\n",
    "log(f\"      Std of features:  {X_train.std():.6f} (should be ~1)\")\n",
    "\n",
    "log(\"\\n[OK] Feature standardization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c92b3d-0283-4298-aa6e-6fd600323214",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Task 05 ‚Äî Multinomial Logistic Regression\n",
    "\n",
    "### Model Definition\n",
    "\n",
    "Given feature vector $z \\in \\mathbb{R}^{768}$, multinomial logistic regression models class probabilities via softmax:\n",
    "\n",
    "$$\n",
    "P(y = k \\mid z) = \\frac{\\exp(w_k^\\top z + b_k)}{\\sum_{j=1}^{5} \\exp(w_j^\\top z + b_j)}, \\quad k \\in \\{1, 2, 3, 4, 5\\}\n",
    "$$\n",
    "\n",
    "### Loss Function (Cross-Entropy)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\sum_{i=1}^{n} \\sum_{k=1}^{5} \\mathbf{1}[y_i = k] \\log P(y = k \\mid z_i; \\theta)\n",
    "$$\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|----------|\n",
    "| `solver` | `lbfgs` | Efficient for multinomial problems |\n",
    "| `max_iter` | 1000 | Ensure convergence |\n",
    "| `class_weight` | `balanced` | Handle class imbalance |\n",
    "| `multi_class` | `multinomial` | Proper softmax (not OvR) |\n",
    "\n",
    "### Evaluation Metric: Balanced Accuracy\n",
    "\n",
    "Standard accuracy can be misleading with imbalanced classes. **Balanced accuracy** averages recall across classes:\n",
    "\n",
    "$$\n",
    "\\text{Balanced Accuracy} = \\frac{1}{K} \\sum_{k=1}^{K} \\text{Recall}_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326ec7af-9c34-4df5-8af6-517f3cb49cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5 ‚Äî MULTINOMIAL LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "\n",
      "[5.1] Training multinomial logistic regression...\n",
      "      Training complete.\n",
      "\n",
      "[5.2] Generating predictions...\n",
      "\n",
      "[5.3] Evaluation Metrics:\n",
      "--------------------------------------------------\n",
      "      Balanced Accuracy (TRAIN): 0.6934\n",
      "      Balanced Accuracy (TEST):  0.4283\n",
      "\n",
      "[5.4] Classification Report (TEST):\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 1      0.728     0.673     0.699       254\n",
      "     Class 2      0.446     0.456     0.451       228\n",
      "     Class 3      0.189     0.252     0.216       123\n",
      "     Class 4      0.309     0.311     0.310       148\n",
      "     Class 5      0.528     0.449     0.486       187\n",
      "\n",
      "    accuracy                          0.464       940\n",
      "   macro avg      0.440     0.428     0.432       940\n",
      "weighted avg      0.483     0.464     0.472       940\n",
      "\n",
      "\n",
      "[5.5] Confusion Matrix (TEST):\n",
      "--------------------------------------------------\n",
      "      Predicted ‚Üí\n",
      "      [[171  60  19   4   0]\n",
      " [ 53 104  48  21   2]\n",
      " [  6  32  31  36  18]\n",
      " [  2  16  29  46  55]\n",
      " [  3  21  37  42  84]]\n",
      "\n",
      "[OK] Model training and evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5 ‚Äî MULTINOMIAL LOGISTIC REGRESSION\n",
    "# ============================================================================\n",
    "log(\"\\n\" + \"=\"*70)\n",
    "log(\"STEP 5 ‚Äî MULTINOMIAL LOGISTIC REGRESSION\")\n",
    "log(\"=\"*70)\n",
    "\n",
    "# ---------------------------------\n",
    "# 5.1 Initialize and Train Model\n",
    "# ---------------------------------\n",
    "log(\"\\n[5.1] Training multinomial logistic regression...\")\n",
    "\n",
    "classifier = LogisticRegression(\n",
    "    solver=\"lbfgs\",           # Quasi-Newton optimization\n",
    "    max_iter=1000,            # Sufficient iterations for convergence\n",
    "    class_weight=\"balanced\",  # Adjust for class imbalance\n",
    "    multi_class=\"multinomial\", # True softmax (not one-vs-rest)\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1                 # Parallelize\n",
    ")\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "log(\"      Training complete.\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 5.2 Generate Predictions\n",
    "# ---------------------------------\n",
    "log(\"\\n[5.2] Generating predictions...\")\n",
    "\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# Also get probability estimates for analysis\n",
    "y_test_proba = classifier.predict_proba(X_test)\n",
    "\n",
    "# ---------------------------------\n",
    "# 5.3 Evaluate Performance\n",
    "# ---------------------------------\n",
    "log(\"\\n[5.3] Evaluation Metrics:\")\n",
    "log(\"-\" * 50)\n",
    "\n",
    "train_ba = balanced_accuracy_score(y_train, y_train_pred)\n",
    "test_ba = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "log(f\"      Balanced Accuracy (TRAIN): {train_ba:.4f}\")\n",
    "log(f\"      Balanced Accuracy (TEST):  {test_ba:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "log(\"\\n[5.4] Classification Report (TEST):\")\n",
    "log(\"-\" * 50)\n",
    "print(classification_report(\n",
    "    y_test, y_test_pred,\n",
    "    digits=3,\n",
    "    target_names=[f\"Class {i}\" for i in range(1, 6)]\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "log(\"\\n[5.5] Confusion Matrix (TEST):\")\n",
    "log(\"-\" * 50)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "log(\"      Predicted ‚Üí\")\n",
    "log(f\"      {cm}\")\n",
    "\n",
    "log(\"\\n[OK] Model training and evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Task 06 ‚Äî Export Model Artifacts\n",
    "\n",
    "To enable inference without retraining, we save:\n",
    "\n",
    "| Artifact | File | Purpose |\n",
    "|----------|------|--------|\n",
    "| Classifier | `classifier.joblib` | Trained logistic regression model |\n",
    "| Scaler | `scaler.joblib` | Fitted StandardScaler (Œº, œÉ) |\n",
    "| Config | `config.json` | Model name, embedding dim, etc. |\n",
    "\n",
    "The DeBERTa model/tokenizer are loaded from HuggingFace at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "export-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6 ‚Äî EXPORT MODEL ARTIFACTS\n",
      "======================================================================\n",
      "\n",
      "[6.1] Classifier saved: ./toxicity_model_artifacts\\classifier.joblib\n",
      "[6.2] Scaler saved: ./toxicity_model_artifacts\\scaler.joblib\n",
      "[6.3] Config saved: ./toxicity_model_artifacts\\config.json\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE ‚Äî ARTIFACT SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Output directory: ./toxicity_model_artifacts\n",
      "\n",
      "Saved files:\n",
      "  - classifier.joblib (30.9 KB)\n",
      "  - config.json (0.5 KB)\n",
      "  - scaler.joblib (18.6 KB)\n",
      "\n",
      "‚úÖ All artifacts saved. Ready for inference.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6 ‚Äî EXPORT MODEL ARTIFACTS\n",
    "# ============================================================================\n",
    "log(\"\\n\" + \"=\"*70)\n",
    "log(\"STEP 6 ‚Äî EXPORT MODEL ARTIFACTS\")\n",
    "log(\"=\"*70)\n",
    "\n",
    "import json\n",
    "\n",
    "# ---------------------------------\n",
    "# 6.1 Save Classifier\n",
    "# ---------------------------------\n",
    "classifier_path = os.path.join(OUTPUT_DIR, \"classifier.joblib\")\n",
    "joblib.dump(classifier, classifier_path)\n",
    "log(f\"\\n[6.1] Classifier saved: {classifier_path}\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 6.2 Save Scaler\n",
    "# ---------------------------------\n",
    "scaler_path = os.path.join(OUTPUT_DIR, \"scaler.joblib\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "log(f\"[6.2] Scaler saved: {scaler_path}\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 6.3 Save Configuration\n",
    "# ---------------------------------\n",
    "config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "    \"num_classes\": 5,\n",
    "    \"class_labels\": [1, 2, 3, 4, 5],\n",
    "    \"class_descriptions\": {\n",
    "        \"1\": \"Non-toxic / Neutral\",\n",
    "        \"2\": \"Mildly problematic\",\n",
    "        \"3\": \"Moderately toxic\",\n",
    "        \"4\": \"Clearly toxic\",\n",
    "        \"5\": \"Severely toxic / Hate speech\"\n",
    "    },\n",
    "    \"training_metrics\": {\n",
    "        \"balanced_accuracy_train\": float(train_ba),\n",
    "        \"balanced_accuracy_test\": float(test_ba)\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = os.path.join(OUTPUT_DIR, \"config.json\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "log(f\"[6.3] Config saved: {config_path}\")\n",
    "\n",
    "# ---------------------------------\n",
    "# 6.4 Summary\n",
    "# ---------------------------------\n",
    "log(\"\\n\" + \"=\"*70)\n",
    "log(\"TRAINING COMPLETE ‚Äî ARTIFACT SUMMARY\")\n",
    "log(\"=\"*70)\n",
    "log(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "log(f\"\\nSaved files:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    fpath = os.path.join(OUTPUT_DIR, f)\n",
    "    size = os.path.getsize(fpath) / 1024\n",
    "    log(f\"  - {f} ({size:.1f} KB)\")\n",
    "\n",
    "log(\"\\n‚úÖ All artifacts saved. Ready for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
